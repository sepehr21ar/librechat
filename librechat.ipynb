{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 dir=rtl align=center style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "LibreChat\n",
    "</font>\n",
    "</h1>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این پروژه می‌خواهیم به بهانه‌ی مروری بر فصل‌های پیشین به طراحی یک ربات گفت‌وگو بپردازیم که براساس داده‌هایی که از منابع مختلف همچون فایل‌های <code>PDF</code> / <code>Epub</code>، صفحات ویکی‌پدیا، وب‌سایت‌ها و غیره جمع‌آوری می‌شوند به پرسش‌های کاربران پاسخ می‌دهد. این داده‌ها به فلسفه‌ی لینوکس، نرم‌افزار آزاد و چهره‌های بزرگی که در آن نقش داشته‌اند مربوط هستند و به همین دلیل نام لیبره‌چت (LibreChat) را برای آن انتخاب کرده‌ایم.\n",
    "<br>\n",
    "همان‌طور که احتمالاً خودتان حدس زده‌اید برای طراحی چنین رباتی نیاز است که یک معماری RAG را پیاده‌سازی کنید. با این حال، انتخاب و اختیار تمام جزئیات و گام‌های آن بر عهده‌ی خودتان است. مهم این است که مدل شما در نهایت بتواند به تعداد خوبی از پرسش‌هایی که در نظر گرفته‌ایم به‌درستی پاسخ دهد. برای آن‌که نتایج به‌دست‌آمده قابل داوری خودکار باشند خروجی‌های مدل نیاز است به‌شکل یک سری عبارات تک یا چند کلمه‌ای تجزیه و ساختاریافته شوند. در بخش تولید خروجی در این‌باره بیشتر صحبت خواهیم کرد و قالب مورد انتظار را شرح خواهیم داد.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "بارگیری داده‌ها\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "همان‌طور که اشاره شد در این پروژه قصد داریم از داده‌هایی با انواع متفاوت و از منابع گوناگون استفاده کنیم. بنابراین در ابتدا نیاز است به‌کمک توابع <code>LangChain</code> متناسب با هر نوع داده نسبت به خوانش ‌آن‌ها اقدام کنید تا تمام داده‌ها به‌شکل سند (<code>Document</code>) در بیایند و بتوانید آن‌ها را مشابه با همدیگر مدیریت کنید. در ادامه داده‌های مورد نیاز این پروژه را با توجه به نوع‌شان تفکیک کرده و ماهیت و منبع آن‌ها را شرح داده‌ایم.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "فایل <code>PDF</code>\n",
    "</font>\n",
    "</h3>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "فایل <code>PDF</code> کتابی با نام «فقط برای تفریح - داستان یک انقلاب اتفاقی» نوشته‌ی لینوس توروالدز، خالق لینوکس و دیوید دیاموند با ترجمه‌ای آزاد از <a href=\"https://jadi.net/\" target=\"_blank\">جادی</a> در پوشه‌ی داده‌های پروژه (<code>data</code>) قرار گرفته است. سعی کنید به‌کمک توابع مخصوص لنگ‌چین جهت خوانش فایل‌های <code>PDF</code> (<a href=\"https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/\" target=\"_blank\">لینک به مستندات</a>) متن این کتاب را به‌شکل سند استخراج کنید.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "<span style=\"color:green\"><b>راهنمایی:</b></span>\n",
    "توجه داشته باشید که هر کدام از توابعی که در <code>LangChain</code> جهت خوانش فایل <code>PDF</code> در نظر گرفته شده از یک کتابخانه‌ی مجزا استفاده می‌کند و معمولاً باید چندین مورد مختلف را آزمایش کرد تا موردی با کیفیت خروجی مطلوب را برگزید. با این حال، یک پیشنهاد می‌تواند استفاده از <code>PyPDFium2Loader</code> باشد. البته اگر می‌خواهید متن کتاب را هر چه بهتر و دقیق‌تر استخراج کنید می‌توانید به‌دلخواه از ابزارها و تکنیک‌های دیگری نیز بهره ببرید. پیشنهاد می‌کنیم با توجه خروجی به‌دست‌آمده و بررسی آن‌ها یک سری پیش‌پردازش‌های متنی را جهت بهبود آن‌ها اعمال کنید. چند ایده درباره‌ی پیش‌پردازش متن در صفحه‌ی پروژه در سامانه‌ی کوئرا نوشته شده است.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "<span style=\"color:orange\"><b>نکته:</b></span>\n",
    "ممکن است با جست‌وجوهای خود به نسخه‌های دیگری از این کتاب همچون فایل <code>Epub</code> نیز برخورد کنید. با این حال پیشنهاد می‌شود که در همین پروژه با چالش‌های خوانش یک فایل <code>PDF</code> فارسی (آن هم فایلی که دیجیتالی نوشته شده است) مواجه شوید تا بتوانید از تجربه‌ی خود در پروژه‌های شخصی‌تان بهره ببرید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sepehr\\miniconda3\\envs\\quera_LLM\\Lib\\site-packages\\pypdfium2\\_helpers\\textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
      "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents saved to processed_docs.json successfully!\n",
      "Processed documents loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "\n",
    "pdf_path = r\"C:\\\\Users\\\\Sepehr\\\\quera_LLM\\\\librechat\\\\librechat\\\\data\\\\justforfun_persian.pdf\"\n",
    "\n",
    "pdf_loader = PyPDFium2Loader(pdf_path)\n",
    "pdf_docs = pdf_loader.load()\n",
    "\n",
    "def replace_white_spaces(text):\n",
    "    text = text.replace(chr(876) + ' ', 'ی ')\n",
    "    text = text.replace(chr(876) + chr(13), 'ی ')\n",
    "    text = text.replace('ͷ', 'ک')\n",
    "    # Added new replacement for 'مͬ'\n",
    "    text = text.replace('مͬ', 'می')\n",
    "    text = text.replace('حروف چینی نسخه لاتک: MXAmin', '')\n",
    "    return text\n",
    "\n",
    "# Un-commented and corrected section to process and save documents\n",
    "processed_docs = []\n",
    "for doc in pdf_docs:\n",
    "    fixed_text = replace_white_spaces(doc.page_content)\n",
    "    processed_docs.append(Document(page_content=fixed_text, metadata=doc.metadata))\n",
    "\n",
    "# Prepare the documents for JSON serialization\n",
    "# We need to convert the list of Document objects to a serializable format (e.g., a list of dictionaries)\n",
    "serializable_docs = [\n",
    "    {\"page_content\": doc.page_content, \"metadata\": doc.metadata}\n",
    "    for doc in processed_docs\n",
    "]\n",
    "\n",
    "\n",
    "# Write the processed documents to the JSON file\n",
    "output_file_path = \"processed_docs.json\"\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(serializable_docs, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Processed documents saved to {output_file_path} successfully!\")\n",
    "\n",
    "# --- Now, load the documents back from the JSON file ---\n",
    "# This part of your code can now run without error\n",
    "try:\n",
    "    with open(output_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        loaded_docs_data = json.load(f)\n",
    "\n",
    "    # Convert the loaded dictionaries back to Document objects\n",
    "    loaded_processed_docs = [\n",
    "        Document(page_content=doc_data[\"page_content\"], metadata=doc_data[\"metadata\"])\n",
    "        for doc_data in loaded_docs_data\n",
    "    ]\n",
    "\n",
    "    print(\"Processed documents loaded successfully!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {output_file_path} was not found.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: The file {output_file_path} is empty or corrupted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages (loaded Document objects): 204\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of pages (loaded Document objects):\", len(loaded_processed_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تولد یک نرد، بخش دوم صفحۀ ١١\n",
      "و او می خواست من را هم در این تجربه شریک کند. همچنین می خواست من را به ریاضی \n",
      "علاقمند کند.\n",
      "پس من را روی زانو هایش می نشاند و از من می خواست تا برنامه هایی که با دقت روی کاغذ\n",
      "نوشته بود را برایش تایپ کنم. می گفت خودش با کامپیوترها راحت نیست. نمی دانم آن محاسبات\n",
      "راجع به چه چیزی بودند و بعید می دانم که آن موقع هیچ درکی از کاری که می کردم هم داشته باشم\n",
      "ولی به هرحال آنجا بودم و به او کمک می کردم. احتمالا کار از حالتی که او خودش به تنهایی\n",
      "برنامه ها را وارد می کرد، خیلی بیشتر طول می کشید. ولی کسی چه می داند؟ من از همان کودکی به\n",
      "صفحه کلید عادت کرده بودم، چیزی که پدربزرگم هیچ وقت امͺان اش را نداشت. بعد از مدرسه\n",
      "یا هر موقع دیͽری که مادرم من را پیش پدربزرگم می گذاشت، مشغول همین کار می شدیم.\n",
      "بعد شروع کردم به خواندن راهنماهای کامپیوتر و وارد کردن برنامه های آماده شده. مثال ها\n",
      "شامل بازی های ساده ای بودند که خودتان می توانستید آن ها را وارد کنید. اگر همه چیز را درست\n",
      "تایپ می کردید، یک آقایی با گرافیک بد، روی صفحه راه می رفت. بعد می توانستید برنامه را عوض\n",
      "کنید تا آقای راه رونده، رنگش عوض شود. شما خودتان می توانستید این کار را بͺنید.\n",
      "این بالاترین لذت بود.\n",
      "شروع کردم به نوشتن برنامه های خودم. اولین برنامه ای که نوشتم، اولین برنامه ای بود که هر\n",
      "کسی می نویسد:\n",
      "10 PRINT \"HELLO\"\n",
      "20 GOTO 10\n",
      "این برنامه دقیقا همان کاری را می کند که انتظار دارید بͺند. روی صفحه می نویسد “سلام” و\n",
      "تا ابد به این کار ادامه می دهد. یا حداقل تا وقتی که شما از شدت سر رفتن حوصله تان، برنامه را\n",
      "قطع کنید.\n",
      "اما این قدم اول است. بعضی ها همین جا متوقف می شوند.برای آن ها این برنامه احمقانه ای\n",
      "است چون “چرا باید کسی علاقمند باشد به میلیون ها کلمه ‘سلام’ خیره شود؟” اما به هرحال این\n",
      "برنامه تقریبا همیشه اولین برنامه در راهنماهایی بود که آن روزها همراه کامپیوترهای شخصی داده\n",
      "می شدند.\n",
      "نکته جادویی اینجا است که شما می توانید این برنامه را تغییر دهید. خواهرم می گوید که من\n",
      "یک تغییر ریشه ای در برنامه دادم تا نسخه دومی بسازم که به جای نوشتن “سلام”، روی صفحه\n",
      "بارها و بارها می نوشت “سارا بهترین است.” در کل من برادر بزرگ تر مهربانی نبودم ولی این ژست\n",
      "برنامه نویس،ی تاثیر زیادی روی خواهرم گذاشت.\n",
      "من این جریان را یادم نیست. هر بار که یک برنامه می نوشتم، آن را فراموش می کردم و سراغ\n",
      "برنامه بعدی می رفتم.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at a random page of the document \n",
    "print(loaded_processed_docs[15].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "لینک وب\n",
    "</font>\n",
    "</h3>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "محتوای دیگری که قصد استفاده از آن را داریم کتاب «لینوکس و زندگی» از <a href=\"https://jadi.net/\" target=\"_blank\">جادی</a> است که به‌صورت آزاد در دسترس است. با این حال، این کتاب نسخه‌ی <code>PDF</code> نداشته و می‌خواهیم آن را مستقیماً از بستر وب بخوانیم. بنابراین نیاز است به‌کمک توابع مرتبط لنگ‌چین، محتوای لینک <a href=\"https://linuxbook.ir/all.html\" target=\"_blank\"><code dir=ltr>https://linuxbook.ir/all.html</code></a> را بارگیری کنید. برای این کار می‌توانید از تابع <code>WebBaseLoader</code> کمک بگیرید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "web_docs = WebBaseLoader(\"https://linuxbook.ir/all.html\").load() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "لینوکس و زندگی | لینوکس و زندگی\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle navigation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "لینوکس و زندگی\n",
      "\n",
      "\n",
      "\n",
      "روی جلد\n",
      "درباره\n",
      "دانلود\n",
      "حمایت\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "لینوکس و زندگی\n",
      "\n",
      "\n",
      "فهرست \n",
      "لینوکس و زندگی. نوشته جادی www.jadi.netبرای دسترسی به آخرین نسخه کتاب در فرمت‌های مختلف و همچنین حمایت مالی از پروژه کتاب‌هایی برای گیک‌ها، به سایت www.jadi.ir و بخش حمایت مراجعه کنید.درباره این کتاب \n",
      "این کتاب سعی می کنه به خواننده ایده‌هایی درمورد زندگی و لینوکس بده. چرا زندگی؟ چون لینوکس یک فلسفه است و براومده از یک جامعه و کسی که می خواد توش موفق باشه با\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(web_docs[0].page_content[:512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "صفحه‌ی ویکی‌پدیا\n",
    "</font>\n",
    "</h3>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "علاوه‌بر محتواهایی که تاکنون در دسترس ما قرار گرفت قصد داریم از محتوای موجود در صفحات ویکی‌پدیای مرتبط با آن‌ها نیز استفاده کنیم. در لیست زیر عنوان چند صفحه‌ی ویکی‌پدیا قرار داده شده که نیاز است با کمک توابعی همچون <code>WikipediaLoader</code> آن‌ها را بارگیری کنید.\n",
    "\n",
    "<ul dir=rtl>\n",
    "<li>ریچارد استالمن</li>\n",
    "<li>لینوس توروالدز</li>\n",
    "<li>لینوکس</li>\n",
    "<li>پروژه گنو</li>\n",
    "<li>نرم‌افزار آزاد</li>\n",
    "<li>بنیاد نرم‌افزار آزاد</li>\n",
    "</ul>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "<span style=\"color:red\"><b>توجه:</b></span>\n",
    "اگر چندین درخواست پشت همدیگر به <i>API</i> ویکی‌پدیا ارسال کنید ممکن است درخواست‌های شما مسدود شود و نیاز است بین درخواست‌های خود اندکی صبر کنید. برای این کار می‌توانید از <code>time.sleep</code> استفاده کرده و بین درخواست‌های خود چند ثانیه وقفه‌ی تصادفی ایجاد کنید. همچنین می‌توانید این کار را در یک حلقه انجام داده و مشخص کنید تا وقتی‌که صفحه‌ی مورد نظر بارگیری نشده چند ثانیه صبر کند و مجدد درخواست دهد.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ صفحه 'ریچارد استالمن' بارگذاری شد. تعداد اسناد: 1\n",
      "✅ صفحه 'لینوس توروالدز' بارگذاری شد. تعداد اسناد: 1\n",
      "✅ صفحه 'لینوکس' بارگذاری شد. تعداد اسناد: 1\n",
      "✅ صفحه 'پروژه گنو' بارگذاری شد. تعداد اسناد: 1\n",
      "✅ صفحه 'نرم‌افزار آزاد' بارگذاری شد. تعداد اسناد: 1\n",
      "✅ صفحه 'بنیاد نرم‌افزار آزاد' بارگذاری شد. تعداد اسناد: 1\n",
      "تعداد کل اسناد: 6\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "import time, random\n",
    "\n",
    "wiki_titles = ['ریچارد استالمن', 'لینوس توروالدز', 'لینوکس', 'پروژه گنو', 'نرم‌افزار آزاد', 'بنیاد نرم‌افزار آزاد']\n",
    "wiki_titles_eng = [\n",
    "    \"Richard Stallman\",\n",
    "    \"Linus Torvalds\",\n",
    "    \"Linux\",\n",
    "    \"GNU Project\",\n",
    "    \"Free software\",\n",
    "    \"Free Software Foundation\"\n",
    "]\n",
    "\n",
    "wiki_docs = []\n",
    "\n",
    "for title in wiki_titles:\n",
    "    loader = WikipediaLoader(query=title, load_max_docs=1, lang='fa')\n",
    "    docs = loader.load()\n",
    "    if docs:\n",
    "        wiki_docs.extend(docs)\n",
    "        print(f\"✅ صفحه '{title}' بارگذاری شد. تعداد اسناد: {len(docs)}\")\n",
    "    else:\n",
    "        print(f\"⚠️ صفحه '{title}' بارگذاری نشد.\")\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "\n",
    "print(f\"تعداد کل اسناد: {len(wiki_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wikipedia pages (loaded Document objects): 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of wikipedia pages (loaded Document objects):\", len(wiki_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "فایل <code>HTML</code>\n",
    "</font>\n",
    "</h3>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "حال بیایید از وب‌سایت خود آقای استالمن هم کمک بگیریم و محتوای آن را در اختیار پروژه‌ی خود قرار دهیم. برای این کار لینک‌های داخلی وب‌سایت <a href=\"https://stallman.org\" target=\"_blank\"><code>https://stallman.org</code></a> را به‌کمک <i>Web Scraping</i> استخراج کرده‌ایم و فایل‌های <code>HTML</code> این صفحات را در پوشه‌ای به همین نام در مسیر داده‌های پروژه (<code>data</code>) ذخیره کرده‌ایم. کد مربوط به بخش استخراج این صفحات صرفاً جهت مطالعه‌ی بیشتر در اختیار شما قرار گرفته و در این مرحله تنها نیاز است که شما فایل‌های <code>HTML</code> را از پوشه‌ی مذکور بخوانید. برای این کار روش‌های متفاوتی وجود دارد اما یک راه استفاده از <code>DirectoryLoader</code> (<a href=\"https://python.langchain.com/v0.2/docs/how_to/document_loader_directory/\" target=\"_blank\">مطالعه‌ی مستندات</a>) و <code>BSHTMLLoader</code> (<a href=\"https://python.langchain.com/v0.2/docs/integrations/document_loaders/bshtml/#loader-features\"  target=\"_blank\">مطالعه‌ی مستندات</a>) به‌عنوان کلاس بارگیری‌کننده‌ی آن (آرگومان <code>loader_cls</code>) است.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<details>\n",
    "<summary dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">مشاهده‌ی کد استخراج داده‌ها</summary>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "\n",
    "# The following code is used to download the HTML content of the Stallman website\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import os\n",
    "\n",
    "def save_html(url, directory):\n",
    "    response = requests.get(url)\n",
    "    # Create a valid filename from the URL\n",
    "    filename = urlparse(url).path.strip('/').replace('/', '_') or 'index'\n",
    "    filepath = os.path.join(directory, f\"{filename}\")\n",
    "    \n",
    "    # Save the HTML content to a file\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(response.text)\n",
    "    print(f\"Saved: {filepath}\")\n",
    "\n",
    "def is_internal_link(href):\n",
    "    # Parse the href to get its components\n",
    "    parsed_href = urlparse(href)\n",
    "    \n",
    "    # Check if the link is internal\n",
    "    return (\n",
    "        (not href.startswith('#')) and # Check it is not an identifier\n",
    "        (not href.startswith('tel:')) and # Check it is not a telephone number\n",
    "        (href.endswith('.html')) and # Check it is not xml\n",
    "        (parsed_href.netloc == \"\" or  # Empty netloc indicates a relative link\n",
    "        \"stallman.org\" in parsed_href.netloc)  # Netloc contains stallman.org\n",
    "    )\n",
    "\n",
    "def extract_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = set()\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        href = a_tag['href']\n",
    "        if is_internal_link(href):\n",
    "            full_url = urljoin(url, href)  # Create a full URL\n",
    "            links.add(full_url)\n",
    "    return links\n",
    "\n",
    "url = \"https://stallman.org\"\n",
    "links = extract_links(url)\n",
    "directory = \"html\"\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save each link's HTML content\n",
    "for link in links:\n",
    "    save_html(link, directory)\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sepehr\\miniconda3\\envs\\quera_LLM\\Lib\\site-packages\\langchain_community\\document_loaders\\html_bs.py:51: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
      "\n",
      "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "\n",
      "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import XMLParsedAsHTMLWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
      "\n",
      "  soup = BeautifulSoup(f, **self.bs_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 صفحه HTML لود شد\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, BSHTMLLoader\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urljoin, urlparse\n",
    "# import os\n",
    "\n",
    "# def save_html(url, directory):\n",
    "#     response = requests.get(url)\n",
    "#     filename = urlparse(url).path.strip('/').replace('/', '_') or 'index'\n",
    "#     filepath = os.path.join(directory, f\"{filename}.html\")\n",
    "#     with open(filepath, 'w', encoding='utf-8') as file:\n",
    "#         file.write(response.text)\n",
    "#     print(f\"Saved: {filepath}\")\n",
    "\n",
    "# def is_internal_link(href):\n",
    "#     parsed_href = urlparse(href)\n",
    "#     return (\n",
    "#         (not href.startswith('#')) and\n",
    "#         (not href.startswith('tel:')) and\n",
    "#         (href.endswith('.html') or href.endswith('/')) and\n",
    "#         (parsed_href.netloc == \"\" or \"stallman.org\" in parsed_href.netloc)\n",
    "#     )\n",
    "\n",
    "# def extract_links(url):\n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     links = set()\n",
    "#     for a_tag in soup.find_all('a', href=True):\n",
    "#         href = a_tag['href']\n",
    "#         if is_internal_link(href):\n",
    "#             full_url = urljoin(url, href)\n",
    "#             links.add(full_url)\n",
    "#     return links\n",
    "\n",
    "# url = \"https://stallman.org\"\n",
    "# links = extract_links(url)\n",
    "# links.add(url)  # اضافه کردن صفحه اصلی\n",
    "# directory = \"html\"\n",
    "# os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# for link in links:\n",
    "#     save_html(link, directory)\n",
    "\n",
    "\n",
    "loader = DirectoryLoader(\"C:\\\\Users\\\\Sepehr\\\\quera_LLM\\\\librechat\\\\librechat\\\\data\\\\html\", glob=\"**/*.html\", loader_cls=BSHTMLLoader)\n",
    "docs = loader.load()\n",
    "print(len(docs), \"صفحه HTML لود شد\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages (loaded Document objects): 179\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of pages (loaded Document objects):\", len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "جداسازی اسناد\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "احتمالاً طبق بررسی اسناد متوجه شده‌اید که برخی از آن‌ها ممکن است طولانی باشند و بهتر است که آن‌ها را به قطعات کوچک‌تری بشکنیم. پس در این مرحله با استفاده از روش‌های جداسازی اسناد به شکستن آن‌ها بپردازید. اگر می‌خواهید این کار را بر روی تمام اسناد خود انجام دهید می‌توانید ابتدا تمام آن‌ها در یک لیست واحد ریخته و سپس تابع مورد نظر را اعمال کنید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of splitted documents: 5192\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document \n",
    "\n",
    "\n",
    "\n",
    "all_docs =  wiki_docs + docs + loaded_processed_docs\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=150)\n",
    "\n",
    "splitted_docs = text_splitter.split_documents(all_docs) \n",
    "\n",
    "print('The number of splitted documents:', len(splitted_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "splitted_docs_data = [\n",
    "    {\"page_content\": doc.page_content, \"metadata\": doc.metadata} for doc in splitted_docs\n",
    "]\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(\"splitted_docs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(splitted_docs_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Documents saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "تعبیه‌سازی و مخزن برداری\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "برای آن‌که بتوانیم اسناد مرتبط با پرسش کاربر را پیدا کردیم یا به‌اصطلاح جست‌وجوی معنایی انجام دهیم نیاز است که آن‌ها را به‌شکل بردارهای عددی تعبیه کرده و در یک مخزن برداری ذخیره کنیم. در این مرحله می‌توانید از هر مدل تعبیه‌سازی که می‌خواهید استفاده کنید. با این حال به چندزبانه بودن اسناد توجه داشته باشید و ترجیحاً از یک مدل چندزبانه استفاده کنید یا این‌که تکنیکی برای رفع این چالش بیابید.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "<span style=\"color:orange\"><b>نکته‌ی ۱:</b></span>\n",
    "اگر قصد دارید از مدل‌های تعبیه‌ساز <i>Cohere</i> استفاده کنید به محدودیت تعداد درخواست ماهانه توجه داشته باشید. ممکن است که اگر چند مرحله این بخش را تکرار کنید حد مجاز تعداد درخواست را رد کنید. یک راه‌حل این است که در صورت وقوع محدودیت از یک حساب جدید استفاده کنید یا این‌که در زمان پیاده‌سازی اولیه و آزمایش کدهای خود از یک مدل دیگر همچون مدل‌های موجود در هاگینگ‌فیس بهره ببرید و وقتی‌که مطمئن شدید همه‌چیز به‌درستی کار می‌کند به‌سراغ آزمایش این مدل بروید.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "<span style=\"color:orange\"><b>نکته‌ی ۲:</b></span>\n",
    "اگر تصمیم گرفتید که از مدل‌های هاگینگ‌فیس بهره ببرید اکیداً پیشنهاد می‌کنیم که کدهای خود را بر بستر گوگل کولب اجرا کنید تا با مشکلات نصب کتابخانه‌ها و همچنین محدودیت‌های سخت‌افزاری مواجه نشوید. در این‌صورت فراموش نکنید که قابلیت <code>GPU</code> کولب را فعال کرده باشید.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "<span style=\"color:red\"><b>توجه:</b></span>\n",
    "در هنگام ساخت مخزن برداری فراموش نکنید که آرگومان <code>persist_directory</code> را مقداردهی کرده باشید تا مطمئن شوید که اسناد و تعبیه‌ها ذخیره می‌شوند. در این‌صورت دیگر در استفاده‌های بعدی از کدهای‌تان نیاز به تعبیه‌سازی مجدد اسناد نخواهید داشت.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sepehr\\AppData\\Local\\Temp\\ipykernel_57976\\4279908110.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# TODO: Define an embedding model and set API key if necessary\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# embedding_model = HuggingFaceEmbeddings(\n",
    "#     model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "# )\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "# from langchain_cohere import CohereEmbeddings\n",
    "# from langchain.vectorstores import Chroma\n",
    "\n",
    "# embeddings = CohereEmbeddings(\n",
    "#     model = \"embed-multilingual-light-v3.0\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector store ساخته و ذخیره شد.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Make a vectorstore and add the documents to it\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    splitted_docs,  \n",
    "    embedding_model,\n",
    "    collection_name = \"Linux_collection\",\n",
    "    persist_directory=\"data/vectorstores2\"  \n",
    ")\n",
    "\n",
    "print(\"✅ Vector store ساخته و ذخیره شد.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector store بارگذاری شد.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# بارگذاری وکتور استور ذخیره‌شده\n",
    "vector_store = Chroma(\n",
    "    persist_directory=\"data/vectorstores\",\n",
    "    collection_name=\"Linux_collection\",\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "print(\"✅ Vector store بارگذاری شد.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5192\n"
     ]
    }
   ],
   "source": [
    "print(vector_store._collection.count())  # تعداد داکیومنت‌ها\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نتایج جستجو برای عبارت 'لینوکس':\n",
      "نتیجه 1:\n",
      "0x680) آتاری، آمیͽا، مͺینتاش) و اسپارک و 32016NS آن را اجرا می کنند. لینوکس شدیدا\n",
      "به 86x80 وابسته است و جای دیͽری ندارد که برود.\n",
      "البته اشتباه نشود. من به خاطر لینوکس خوشحالم چون تمام افرادی که سعی م\n",
      "-----\n",
      "نتیجه 2:\n",
      "صفحۀ ١۴٨ فرش قرمز، بخش نهم\n",
      "بخش نهم\n",
      "آیا انقلاب لینوکس به پایان رسیده است؟\n",
      "نوشته اسͺات بریناتو، Week PC\n",
      "“ از تماس شما ممنونم. انقلاب به پایان رسیده است. اگر\n",
      "اطلاعات بیشتری در مورد لینوکس می خواهید،\n",
      "-----\n",
      "نتیجه 3:\n",
      "در دست شرکت هایی همچون مایͺروسافت یا شرکت های صاحب یونیͺس باشد. شما می توانید\n",
      "کارهایی با لینوکس بͺنید که با هیچ سیستم عامل دیͽری نمی توانید بͺنید. اولین کاربران لینوکس\n",
      "به این دلیل لینوکس را انتخاب ک\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# query = \"نرم‌افزار آزاد\"\n",
    "query = \"لینوکس\"\n",
    "results = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"نتایج جستجو برای عبارت '{query}':\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"نتیجه {i}:\")\n",
    "    print(doc.page_content[:200])  \n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "مدل پرسش‌وپاسخ\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "اکنون می‌توانید به هر روشی که علاقه دارید یک زنجیره‌ی <code>LangChain</code> طراحی کنید که بتواند به پرسش کاربر در قالب خواسته‌شده پاسخ دهد. قاعدتاً این زنجیره باید شامل بازیابی اسناد به‌کمک مخزن برداری نیز باشد. همچنین می‌توانید یک دستور سفارشی‌شده و مناسب با خواسته‌های مسئله بنویسید و سعی کنید با بهبود آن کیفیت پاسخ‌های مدل را نیز افزایش دهید. با این حال، مهم‌ترین نکته در این بخش فرمت خروجی مدل است که در ادامه به شرح آن می‌پردازیم. برای رعایت این فرمت احتمالاً نیاز به استفاده از تجزیه‌کننده (Parser) در زنجیره‌ی خود خواهید داشت.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<h3 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "سوالات و فرمت خروجی\n",
    "</font>\n",
    "</h3>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "با این‌که مدل شما قادر است به‌صورت کامل‌تر و نزدیک‌تر به زبان انسان به پرسش‌های کاربر پاسخ دهد اما از آن‌جا که  بتوانیم عملکرد مدل شما را به‌صورت خودکار توسط سیستم داوری ارزیابی کنیم یک سری سوال مشخص در اختیارتان قرار داده‌ایم که پاسخ‌هایی تک یا چند کلمه‌ای دارند. سیستم داوری وجود عبارت پاسخ درست (یا معادل‌های آن) را در خروجی‌های شما جست‌وجو می‌کند و اگر آن را پیدا کند به‌عنوان پاسخی درست در نظر می‌گیرد. با این حال توجه داشته باشید اگر خروجی مدل شما <u>بیش از ۴ کلمه</u> باشد به‌عنوان یک متن طولانی در نظر گرفته می‌شود و <span style=\"color:red\">نادرست</span> تشخیص داده می‌شود زیرا تمام پاسخ‌ها کوتاه و مشخص هستند.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "خروجی نهایی شما به‌ازای هر سوال باید یک دیکشنری شامل دو کلید <code>question_number</code> و <code>answer</code> باشد که به‌ترتیب از جنس <code>int</code> و <code>str</code> هستند. در این دیکشنری شماره‌ی سوال به‌عنوان مقدار کلید <code>question_number</code> و پاسخ به‌عنوان مقدار کلید <code>answer</code> قرار می‌گیرد. به‌عنوان مثال اگر پاسخ پرسش شماره‌ی ۱۲، کوئرا باشد دیکشنری مربوط به آن به‌صورت زیر خواهد بود:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"question_number\": 12,\n",
    "    \"answer\": \"کوئرا\"\n",
    "}\n",
    "```\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "در انتهای نت‌بوک تمام این دیکشنری‌ها (از متغیر <code>answer1</code> تا <code>answer16</code>) را داخل یک لیست واحد ریخته و به‌صورت یک فایل <code>JSON</code> ذخیره خواهیم کرد تا بتوانید آن را در سامانه‌ی داوری آپلود کرده و نتایج کار خود را مشاهده کنید.\n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "from langchain_cohere import ChatCohere\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "if not COHERE_API_KEY:\n",
    "    try:\n",
    "        import getpass\n",
    "        COHERE_API_KEY = getpass.getpass(\"COHERE_API_KEY: \")\n",
    "    except Exception:\n",
    "        COHERE_API_KEY = \"\"\n",
    "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"پرسش ۱: اولین سیستم‌عامل یونیکس چه بود؟\", \"question_number\": 1, \"answer\": \"UNIX\"},\n",
    "    {\"input\": \"پرسش ۲: بنیان‌گذار نرم‌افزار آزاد چه کسی است؟\", \"question_number\": 2, \"answer\": \"ریچارد استالمن\"},\n",
    "    {\"input\": \"پرسش ۳: زبان برنامه‌نویسی پایتون توسط چه کسی توسعه یافت؟\", \"question_number\": 3, \"answer\": \"خیدو فان روسوم\"},\n",
    "    {\"input\": \"پرسش ۱۶: در بیانیه‌ی هکرها گفته شده که جرم آن‌ها در یک کلمه چیست؟\", \"question_number\": 16, \"answer\": \"داشتن روحیه‌ی کنجکاوی\"}\n",
    "]\n",
    "examples_str_que = \"\\n\".join(ex[\"input\"] for ex in examples)\n",
    "examples_str_ans = \"\\n\".join(\n",
    "    json.dumps({\"question_number\": ex[\"question_number\"], \"answer\": ex[\"answer\"]}, ensure_ascii=False)\n",
    "    for ex in examples\n",
    ")\n",
    "\n",
    "PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "شما یک دستیار پاسخ‌گوی دقیق هستید که فقط بر اساس «اطلاعات بازیابی‌شده (context)» پاسخ می‌دهید (Closed-Book).\n",
    "قواعد بسیار مهم:\n",
    "1) فقط JSON استاندارد بازگردانید (با کوتیشن دوتایی)، بدون هیچ متن اضافی قبل/بعد.\n",
    "2) قالب دقیق: {{\"question_number\": <int> , \"answer\": \"<str حداکثر 4 کلمه>\"}}\n",
    "3) اگر در context پاسخ معتبری نبود، \"answer\" باید \"ناپیدا\" باشد.\n",
    "4) عدد/درصد را فقط به صورت عدد برگردانید (بدون کلمه اضافی).\n",
    "5) پاسخ باید هم‌زبان با پرسش باشد (اگر پرسش فارسی است، پاسخ فارسی).\n",
    "\n",
    "مثال پرسش‌ها:\n",
    "{examples_que}\n",
    "\n",
    "مثال پاسخ‌های صحیح:\n",
    "{examples_ans}\n",
    "\n",
    "اطلاعات بازیابی‌شده (context):\n",
    "{context}\n",
    "\n",
    "پرسش کاربر:\n",
    "{question}\n",
    "\n",
    "فقط JSON معتبر را بازگردان.\n",
    "\"\"\"\n",
    ")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",  # یا similarity\n",
    "    search_kwargs={\n",
    "        \"k\": 170,\n",
    "        \"fetch_k\": 400\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "reranker = CohereRerank(model=\"rerank-multilingual-v3.0\", top_n=88)\n",
    "reranked_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "llm = ChatCohere(\n",
    "    model=\"command-r-plus\", \n",
    "    temperature=0.2, \n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "def _extract_question_number(query: str) -> int | None:\n",
    "    \"\"\"\n",
    "    استخراج امن شماره پرسش از متن کاربر:\n",
    "    پشتیبانی از «پرسش ۱۲» یا «Question 12».\n",
    "    \"\"\"\n",
    "    m = re.search(r\"(?:پرسش|question)\\s*[:\\-]?\\s*(\\d+)\", query, flags=re.IGNORECASE)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def _join_docs(docs) -> str:\n",
    "    \"\"\"\n",
    "    کانتکست تمیز با delimiter بین اسناد.\n",
    "    \"\"\"\n",
    "    parts: List[str] = []\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        txt = (d.page_content or \"\").strip()\n",
    "        if not txt:\n",
    "            continue\n",
    "        parts.append(f\"---[DOC {i}]---\\n{txt}\")\n",
    "    return \"\\n\\n\".join(parts) if parts else \"\"\n",
    "\n",
    "def _limit_to_4_words(s: str) -> str:\n",
    "    words = s.strip().split()\n",
    "    return \" \".join(words[:4])\n",
    "\n",
    "def _parse_llm_output(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    پارسر مقاوم:\n",
    "    1) تلاش JSON مستقیم\n",
    "    2) تلاش literal_eval برای دیکشنری‌های ' با کوتیشن تکی\n",
    "    3) تلاش regex برای بیرون کشیدن بلاک {...}\n",
    "    در پایان، answer را به 4 کلمه محدود و \\u200c را حذف می‌کند.\n",
    "    \"\"\"\n",
    "    cleaned = text.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip().replace(\"\\u200c\", \"\")\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(cleaned)\n",
    "    except Exception:\n",
    "        try:\n",
    "            obj = ast.literal_eval(cleaned)\n",
    "        except Exception:\n",
    "            m = re.search(r\"\\{.*\\}\", cleaned, flags=re.DOTALL)\n",
    "            if not m:\n",
    "                raise ValueError(\"No JSON-like object found\")\n",
    "            block = m.group(0)\n",
    "            try:\n",
    "                obj = json.loads(block)\n",
    "            except Exception:\n",
    "                obj = ast.literal_eval(block)\n",
    "\n",
    "    if not isinstance(obj, dict):\n",
    "        raise ValueError(\"Parsed object is not a dict\")\n",
    "\n",
    "    if \"question_number\" in obj:\n",
    "        try:\n",
    "            obj[\"question_number\"] = int(obj[\"question_number\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if \"answer\" in obj and isinstance(obj[\"answer\"], str):\n",
    "        obj[\"answer\"] = _limit_to_4_words(obj[\"answer\"].replace('\"', \"\").strip())\n",
    "\n",
    "    return obj\n",
    "def rag_chain(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    ورودی: query (پرسش کاربر)\n",
    "    خروجی: دیکشنری {\"question_number\": int, \"answer\": str}\n",
    "    \"\"\"\n",
    "    qnum = _extract_question_number(query)\n",
    "\n",
    "    retrieved_docs = reranked_retriever.get_relevant_documents(query)\n",
    "    if not retrieved_docs:\n",
    "        return {\"question_number\": qnum if qnum is not None else -1, \"answer\": \"ناپیدا\"}\n",
    "\n",
    "    context = _join_docs(retrieved_docs)\n",
    "\n",
    "    prompt_messages = PROMPT.format_messages(\n",
    "        examples_que=examples_str_que,\n",
    "        examples_ans=examples_str_ans,\n",
    "        context=context,\n",
    "        question=query\n",
    "    )\n",
    "\n",
    "    raw = llm.invoke(prompt_messages)\n",
    "    raw_text = getattr(raw, \"content\", raw)  \n",
    "\n",
    "    try:\n",
    "        parsed = _parse_llm_output(raw_text)\n",
    "    except Exception:\n",
    "        return {\n",
    "            \"question_number\": qnum if qnum is not None else -1,\n",
    "            \"answer\": \"ناپیدا\"\n",
    "        }\n",
    "\n",
    "    if qnum is not None:\n",
    "        parsed[\"question_number\"] = qnum\n",
    "\n",
    "    if not parsed.get(\"answer\"):\n",
    "        parsed[\"answer\"] = \"ناپیدا\"\n",
    "\n",
    "    parsed[\"answer\"] = _limit_to_4_words(parsed[\"answer\"])\n",
    "\n",
    "    return parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_number': 1, 'answer': 'ترنسمتا'}\n"
     ]
    }
   ],
   "source": [
    "query = \"پرسش ۱: توروالدز برای کار در چه موسسه‌ای دانشگاه هلسینکی را ترک کرد\"\n",
    "answer1 = rag_chain(query)\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_number': 2, 'answer': 'دانشگاه آمستردام'}\n"
     ]
    }
   ],
   "source": [
    "query = \"پرسش ۲: آندرو تاننباوم استاد کدام دانشگاه است؟\"\n",
    "answer2 = rag_chain(query)\n",
    "print(answer2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_number': 3, 'answer': '2'}\n"
     ]
    }
   ],
   "source": [
    "query = \"پرسش ۳: در سال ۲۰۰۶ چند درصد از هسته لینوکس توسط توروالدز نوشته شد (به عدد)؟\"\n",
    "answer3 = rag_chain(query)\n",
    "print(answer3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_number': 4, 'answer': 'ریچارد استالمن'}\n"
     ]
    }
   ],
   "source": [
    "query = \"پرسش ۴: چه کسی بنیاد نرم‌افزارهای آزاد را بنا نهاد؟\"\n",
    "answer4 = rag_chain(query)\n",
    "print(answer4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_number': 5, 'answer': 'وسچستر کانتی'}\n"
     ]
    }
   ],
   "source": [
    "query = \"پرسش ۵: ریچارد استالمن در ۲۱ سالگی در کدام شرکت کار می‌کرد؟\"\n",
    "answer5 = rag_chain(query)\n",
    "print(answer5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_number': 6, 'answer': 'لیسپ'}\n"
     ]
    }
   ],
   "source": [
    "query = \"پرسش ۶: یکی از مشهورترین پروژه‌هایی که در ابتدا پروژه‌ی آزاد و آکادمیک بود اما بعد وارد محیط بسته‌ی تجاری شد چه بود؟\"\n",
    "answer6 = rag_chain(query)\n",
    "print(answer6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_number': 7, 'answer': 'چین'}\n"
     ]
    }
   ],
   "source": [
    "query = \"پرسش ۷: لینکدین در سانسور کردن حساب‌ها به درخواست چه کشوری مشهور است؟\"\n",
    "answer7 = rag_chain(query)\n",
    "print(answer7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_number': 8, 'answer': 'OpenStreetMap'}\n"
     ]
    }
   ],
   "source": [
    "query = \"پرسش ۸: ریچارد استالمن پیشنهاد می‌کند به‌جای گوگل مپ از چه سرویسی استفاده کنیم؟\"\n",
    "answer8 = rag_chain(query)\n",
    "print(answer8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_number': 9, 'answer': 'آزادی اجرا'}\n"
     ]
    }
   ],
   "source": [
    "query = \"پرسش ۹: آزادی صفرم در نرم‌افزار آزاد چه عنوانی دارد؟\"\n",
    "answer9 = rag_chain(query)\n",
    "print(answer9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_number': 10, 'answer': 'خیر'}\n"
     ]
    }
   ],
   "source": [
    "query = \"پرسش ۱۰: آیا یک نرم‌افزار آزاد لزوماً رایگان است (بله یا خیر)؟\"\n",
    "answer10 = rag_chain(query)\n",
    "print(answer10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_number': 11, 'answer': 'پوسیͺس'}\n"
     ]
    }
   ],
   "source": [
    "query = \"پرسش ۱۱: استاندارد ناظر بر فایل‌ها و دایرکتوری‌ها به‌اختصار چه نامیده می‌شود؟\"\n",
    "answer11 = rag_chain(query)\n",
    "print(answer11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_number': 12, 'answer': 'سلام'}\n"
     ]
    }
   ],
   "source": [
    "query = \"پرسش ۱۲: اولین ریپلای به ایمیل درخواست کار چیست؟\"\n",
    "answer12 = rag_chain(query)\n",
    "print(answer12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_number': 13, 'answer': 'بایاس انتخاب'}\n"
     ]
    }
   ],
   "source": [
    "query = \"پرسش ۱۳: اگر امروز که از شنبه ورزش می‌کنم در واقع دچار چه بایاسی شده‌ایم؟\"\n",
    "answer13 = rag_chain(query)\n",
    "print(answer13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_number': 14, 'answer': 'برنامهنویسی'}\n"
     ]
    }
   ],
   "source": [
    "query = \"پرسش ۱۴: دنبال یاد گرفتن کدوم یکی باشیم: برنامه‌نویسی یا دستور زبان یک زبان خاص؟\"\n",
    "answer14 = rag_chain(query)\n",
    "print(answer14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_number': 15, 'answer': 'ناپیدا'}\n"
     ]
    }
   ],
   "source": [
    "query = \"پرسش ۱۵: اگه هدف‌مون اینه که بریم گوگل کار کنیم اول از همه چه‌چیزی رو سرچ کنیم؟\"\n",
    "answer15 = rag_chain(query)\n",
    "print(answer15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_number': 16, 'answer': 'داشتن روحیهی کنجکاوی'}\n"
     ]
    }
   ],
   "source": [
    "query = \"پرسش ۱۶: در بیانیه‌ی هکرها گفته شده که جرم آن‌ها در یک کلمه چیست؟\"\n",
    "answer16 = rag_chain(query)\n",
    "print(answer16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "نحوه‌ی امتیازدهی\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "برای آن‌که بتوانید با <span style=\"color:green\">موفقیت</span> امتیاز این پروژه را کسب کنید نیاز است که مدل شما از بین این ۱۶ پرسش حداقل به <u>۱۲ مورد</u> پاسخ درست داده باشد.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "<b>سلول جواب‌ساز</b>\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "در سلول‌های بعدی فایل‌های پاسخ شما را تولید می‌کنیم تا بتوانید در نهایت یک فایل فشرده با نام <code>result.zip</code> تحویل بگیرید که حاوی فایل‌های مورد نیاز جهت داوری خواهد بود. ابتدا نیاز است که پاسخ‌های شما یعنی متغیرهای <code>answer1</code> تا <code>answer16</code> را داخل یک لیست واحد ذخیره کنیم. سپس این لیست را به‌صورت یک فایل <code>JSON</code> ذخیره می‌کنیم. لطفاً کد زیر را <span style=\"color:red\">بدون تغییر</span> اجرا کنید.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    برای ساخته‌شدن فایل <code>result.zip</code> سلول زیر را اجرا کنید. توجه داشته باشید که پیش از اجرای سلول زیر تغییرات اعمال شده در نت‌بوک را ذخیره کرده باشید (<code>ctrl+s</code>) تا در صورت نیاز به پشتیبانی امکان بررسی کد شما وجود داشته باشد. همچنین اگر از گوگل کولب استفاده می‌کنید، در صورت نیاز به پشتیبانی حتماً آخرین نسخه از نت‌بوک را به‌صورت دستی دانلود کرده و داخل فایل ارسالی قرار دهید یا لینک کولب را با ما به‌اشتراک بگذارید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Paths:\n",
      "['answers.json', 'C:\\\\\\\\Users\\\\\\\\Sepehr\\\\\\\\quera_LLM\\\\\\\\librechat\\\\\\\\librechat\\\\\\\\librechat.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "answers: List[Dict[str, Any]] = []\n",
    "\n",
    "for i in range(1, 17):\n",
    "    ans = globals().get(f'answer{i}')\n",
    "    \n",
    "    if ans is not None:\n",
    "        if isinstance(ans, dict):\n",
    "            answers.append(ans)\n",
    "        else:\n",
    "            print(f\"⚠️ Warning: answer{i} is not a dictionary. Storing as string.\")\n",
    "            answers.append({\"answer\": str(ans)})\n",
    "\n",
    "with open('answers.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(answers, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "notebook_path = r\"C:\\\\Users\\\\Sepehr\\\\quera_LLM\\\\librechat\\\\librechat\\\\librechat.ipynb\"\n",
    "\n",
    "def compress(file_names: List[str]):\n",
    "    \"\"\"\n",
    "    فایل‌های مشخص‌شده را فشرده کرده و در یک فایل ZIP ذخیره می‌کند.\n",
    "    \"\"\"\n",
    "    print(\"File Paths:\")\n",
    "    print(file_names)\n",
    "    compression = zipfile.ZIP_DEFLATED\n",
    "    with zipfile.ZipFile(\"result.zip\", mode=\"w\") as zf:\n",
    "        for file_name in file_names:\n",
    "            if os.path.exists(file_name):\n",
    "                zf.write(file_name, os.path.basename(file_name), compress_type=compression)\n",
    "            else:\n",
    "                print(f\"⚠️ Missing file: {file_name}\")\n",
    "\n",
    "file_names = ['answers.json', notebook_path]\n",
    "compress(file_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "اکنون کافیست فایل <code>result.zip</code> را داخل سامانه داوری ارسال کنید تا امتیاز شما محاسبه شود.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=rtl align=center style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "© <a href=\"https://quera.org/\" target=\"_blank\">کوئرا | Quera</a>\n",
    "</font>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quera_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
